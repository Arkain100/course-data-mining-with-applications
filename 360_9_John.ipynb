{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For this homework assignment, you are to submit a **single** ipynb file. Use the provided ipynb file to keep the same formatting for each question. In the ipynb file name, replace \"NAME\" with your first name. Unless otherwise specified, present your code as well as the output in your report. It is the student's responsibility to make sure the ipynb file runs when submitted. This assignment is worth 59 points."
      ],
      "metadata": {
        "id": "HTbJJTAVA2vA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1: Cross-Validation *(21 points)*"
      ],
      "metadata": {
        "id": "-v3gtAD1DueL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To investigate cross-validation, we will look at the `housing` data set which provides information from the 1990 US Census on housing in California. The 20,000+ observations have features like location, housing, age, and population. The target variable is the median house value for a Californian district.\n",
        "\n",
        "This is an example of regression, as opposed to classification as we have primarily studied.\n",
        "\n",
        "Convert the inputs of the `housing` data set into a `pandas` data frame so that you can look at the variables. *(2 points)*"
      ],
      "metadata": {
        "id": "ox6ntstADyVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()"
      ],
      "metadata": {
        "id": "PE3M4H8LGyhh"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "house_data=pd.DataFrame(housing.data)"
      ],
      "metadata": {
        "id": "kunS2T2yPyU1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data frame (inputs) and the target (outputs) into three sets: training set, validation set, test set as we did in class:\n",
        "\n",
        "- First as a trainval set and test set\n",
        "- Second as a train set and validate set\n",
        "\n",
        "**Reminder to never use the test set until the very final step. We never want to tell our models what the test set is while we are finetuning the model.**  *(5 points)*"
      ],
      "metadata": {
        "id": "rJK6Cj9NHY3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "(X_trainval, X_test, Y_trainval, Y_test)=train_test_split(house_data, housing.target)\n",
        "(splitx_train,splitx_val,splity_train,splity_val)=train_test_split(X_trainval,Y_trainval)\n"
      ],
      "metadata": {
        "id": "-yY__8tWHas8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to perform the regression, we will use a Ridge linear regression model. Fit a ridge model (without any adjusting of parameters) to the training set and record it's accuracy on the validation set. *(3 points)*\n",
        "\n",
        "*Reminder that regression's default score is $R^2$ which has domain $(-\\infty,1]$.*"
      ],
      "metadata": {
        "id": "F0ZsbnjThH3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "house_ridge = Ridge().fit(splitx_train,splity_train)\n",
        "house_ridge.score(splitx_val, splity_val)"
      ],
      "metadata": {
        "id": "iv1ZXHUkhFHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c32fc84-bd98-4bac-bc09-016e263e39ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6235308901361161"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform a stratified fold on the trainval set retuning all of the scores along with the average score. Is this score better than our original score without cross-validating?  *(6 points)*"
      ],
      "metadata": {
        "id": "rbd7wC7ghqps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "strat_fold = cross_val_score(Ridge(), housing.data,housing.target)\n",
        "print(strat_fold)\n",
        "np.mean(strat_fold)"
      ],
      "metadata": {
        "id": "BfHGmr4UIRWb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b14969-8e06-4a71-8ddc-f29eb116009b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.54878594 0.46817341 0.55078466 0.53693584 0.66053068]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.5530421056931834)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original score is better."
      ],
      "metadata": {
        "id": "k3e6YgM9Vhor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increase the number of folds to 10. Return all scores along with the average score and determine if the accuracy improved or worsened. What does this tell you about the sensitivity of the model to the actual train-validate split? *(3 points)*"
      ],
      "metadata": {
        "id": "fvfe5Dq0h9DS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strat_fold10 = cross_val_score(Ridge(), housing.data,housing.target, cv=10)\n",
        "print(strat_fold10)\n",
        "np.mean(strat_fold10)\n"
      ],
      "metadata": {
        "id": "Wc6DbqYxLe4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f8a671-c27c-45f0-ba7d-fed3d81ae961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.482818   0.61412011 0.42268645 0.48182494 0.55703274 0.54134247\n",
            " 0.47497151 0.45838648 0.48177509 0.59533218]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.5110289965995403)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy worsened. This indicates that the model is more sensitive to the training and validation set in the actuatl train-validate split and less sensitive to the further splits."
      ],
      "metadata": {
        "id": "uP16ANuVWM75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt to run a `LeavePOut` fold using $p=1000$. Theoretically we can do this, but in practicality we can't in this case. When you've given up, stop the computation and explain why the computations never stopped. *(2 points)*"
      ],
      "metadata": {
        "id": "WiiraWP2iLw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import LeavePOut\n",
        "leave=LeavePOut(p=1000)\n",
        "strat_leave = cross_val_score(Ridge(), housing.data,housing.target,cv=leave)\n",
        "print(strat_leave)\n",
        "np.mean(strat_leave)"
      ],
      "metadata": {
        "id": "wIl43BqnLvTb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "2d963a8f-c6aa-42df-d9bb-26c51dcfda9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-98d1ba8086dc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLeavePOut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLeavePOut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstrat_leave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRidge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhousing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhousing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrat_leave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrat_leave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    412\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    413\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m         )\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0;31m# when X is sparse we only remove offset from y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m         X, y, X_offset, y_offset, X_scale = _preprocess_data(\n\u001b[0m\u001b[1;32m    957\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m_preprocess_data\u001b[0;34m(X, y, fit_intercept, copy, copy_y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msupported_float_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# error message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mfirst_pass_isfinite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfirst_pass_isfinite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2387\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m     return _wrapreduction(\n\u001b[0m\u001b[1;32m   2390\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m         \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The computation never stopped because it generates so many different models of 1000 points that it would take too long to run pratically."
      ],
      "metadata": {
        "id": "csJELKN0YwS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2: Grid Search *(21 points)*"
      ],
      "metadata": {
        "id": "v8Th0qemikDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continue to use the `housing` data set.\n",
        "\n",
        "Using a for loop, perform a grid search (with cross-validation) with the list of parameter values for the regularization $\\alpha$ in the Ridge model.\n",
        "\n",
        "You should print all of the average scores in an array, the best score, and the best parameter. Also make sure you're using the appropriate data sets (trainval, train, val, or test)! *(7 points)*"
      ],
      "metadata": {
        "id": "Kzr_5oD0ipLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "params = [0.01,0.05,0.1,0.5,1,5,10,20,50,100]\n",
        "\n",
        "arrays_mean=[]\n",
        "best_score = 0\n",
        "best_para = None\n",
        "\n",
        "for i in params:\n",
        "  ral=Ridge(alpha=i)\n",
        "  grid_cross=cross_val_score(ral,X_trainval,Y_trainval)\n",
        "  mean_score=np.mean(grid_cross)\n",
        "  arrays_mean.append(np.mean(grid_cross))\n",
        "\n",
        "\n",
        "  if mean_score > best_score:\n",
        "      best_score = mean_score\n",
        "      best_para = i\n",
        "print(np.array(arrays_mean))\n",
        "print(\"besst Score:\",best_score)\n",
        "print(\"best parameter:\",best_para)"
      ],
      "metadata": {
        "id": "tWkNYoCHRy9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92a1f478-6625-4b58-923d-f815dc12651f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.60418138 0.60418215 0.60418312 0.60419079 0.60420031 0.60427322\n",
            " 0.60435668 0.60450018 0.60477736 0.60488876]\n",
            "besst Score: 0.6048887561125514\n",
            "best parameter: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify that the `GridSearchCV` function perform like your for loop by finding the best parameters and score for the same list of parameters. *(5 points)*"
      ],
      "metadata": {
        "id": "1mf1i6lajMrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params1 = {'alpha': [0.01,0.05,0.1,0.5,1,5,10,20,50,100]}\n",
        "\n",
        "grid_searched=GridSearchCV(Ridge(),params1,cv=5)\n",
        "grid_searched.fit(X_trainval,Y_trainval)\n",
        "print(grid_searched.best_score_)\n",
        "print(grid_searched.best_params_)\n"
      ],
      "metadata": {
        "id": "trslWdNyTssd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ede70c8-9543-4d3c-bf8a-1f176aef96b4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6048887561125514\n",
            "{'alpha': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the information about the best parameters, narrow down the actual best parameter value by adding in values into the `params` array and recomputing the grid search.\n",
        "\n",
        "You should have accuracy up to two decimal places. *(4 points)*"
      ],
      "metadata": {
        "id": "BxhTXvXTjaJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params2 = {'alpha': [0.01,0.05,0.1,0.5,0.6,0.7,0.8,0.9,1,1.5,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,325,30,35,40,45,50,60,70,80,90,91,92,93,94,95,96,97,98,99,100,110,125,150,175,200]}\n",
        "grid_searched_best=GridSearchCV(Ridge(),params2,cv=5)\n",
        "grid_searched_best.fit(X_trainval,Y_trainval)\n",
        "print(grid_searched_best.best_score_)\n",
        "print(grid_searched_best.best_params_)\n"
      ],
      "metadata": {
        "id": "X1SfnyCKX4cF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dfe4eab-9225-4647-cd00-1ad90be74fcd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.60489243063517\n",
            "{'alpha': 92}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, display the results of your final grid search as a data frame. *(2 points)*"
      ],
      "metadata": {
        "id": "Wfnr7rQJj6ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(grid_searched_best.cv_results_)"
      ],
      "metadata": {
        "id": "0l3kjNwKUqYC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8fb4a9e1-db0a-4246-d7f6-fb84d2645905"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  param_alpha  \\\n",
              "0        0.005677      0.001539         0.002475        0.000069         0.01   \n",
              "1        0.004404      0.000071         0.002369        0.000079         0.05   \n",
              "2        0.005297      0.001720         0.002424        0.000128         0.10   \n",
              "3        0.004544      0.000110         0.002454        0.000096         0.50   \n",
              "4        0.004199      0.000051         0.002292        0.000036         0.60   \n",
              "5        0.004152      0.000068         0.002246        0.000085         0.70   \n",
              "6        0.004183      0.000100         0.002318        0.000157         0.80   \n",
              "7        0.004026      0.000031         0.002168        0.000060         0.90   \n",
              "8        0.003961      0.000116         0.002166        0.000052         1.00   \n",
              "9        0.004106      0.000103         0.002160        0.000033         1.50   \n",
              "10       0.003929      0.000067         0.002099        0.000004         5.00   \n",
              "11       0.003974      0.000118         0.002122        0.000053         6.00   \n",
              "12       0.004137      0.000081         0.002281        0.000108         7.00   \n",
              "13       0.004151      0.000086         0.002185        0.000014         8.00   \n",
              "14       0.004324      0.000124         0.002294        0.000158         9.00   \n",
              "15       0.004521      0.000681         0.002453        0.000124        10.00   \n",
              "16       0.004187      0.000075         0.002239        0.000092        11.00   \n",
              "17       0.004191      0.000159         0.002197        0.000032        12.00   \n",
              "18       0.004125      0.000089         0.002163        0.000026        13.00   \n",
              "19       0.004158      0.000127         0.002293        0.000130        14.00   \n",
              "20       0.004193      0.000116         0.002313        0.000182        15.00   \n",
              "21       0.004875      0.000940         0.003335        0.001316        16.00   \n",
              "22       0.005811      0.001476         0.002596        0.000576        17.00   \n",
              "23       0.005102      0.000961         0.003399        0.000871        18.00   \n",
              "24       0.004732      0.000904         0.002269        0.000093        19.00   \n",
              "25       0.004635      0.000839         0.002275        0.000101        20.00   \n",
              "26       0.004658      0.000660         0.002236        0.000037       325.00   \n",
              "27       0.004244      0.000107         0.002247        0.000059        30.00   \n",
              "28       0.004172      0.000155         0.002168        0.000024        35.00   \n",
              "29       0.004108      0.000055         0.002215        0.000087        40.00   \n",
              "30       0.004367      0.000247         0.002205        0.000049        45.00   \n",
              "31       0.004214      0.000117         0.002256        0.000072        50.00   \n",
              "32       0.004452      0.000150         0.002380        0.000059        60.00   \n",
              "33       0.004497      0.000465         0.002884        0.000637        70.00   \n",
              "34       0.005124      0.000537         0.002636        0.000090        80.00   \n",
              "35       0.004754      0.000085         0.003021        0.000592        90.00   \n",
              "36       0.004565      0.000225         0.002439        0.000137        91.00   \n",
              "37       0.004197      0.000083         0.002211        0.000059        92.00   \n",
              "38       0.004389      0.000384         0.002363        0.000179        93.00   \n",
              "39       0.004463      0.000333         0.002331        0.000114        94.00   \n",
              "40       0.004116      0.000123         0.002205        0.000053        95.00   \n",
              "41       0.004552      0.000120         0.002519        0.000172        96.00   \n",
              "42       0.004664      0.000067         0.002740        0.000461        97.00   \n",
              "43       0.005106      0.000464         0.002634        0.000082        98.00   \n",
              "44       0.004784      0.000293         0.002513        0.000147        99.00   \n",
              "45       0.004747      0.000228         0.002738        0.000857       100.00   \n",
              "46       0.004535      0.000312         0.002527        0.000179       110.00   \n",
              "47       0.005942      0.001890         0.002733        0.000299       125.00   \n",
              "48       0.004887      0.000212         0.002578        0.000065       150.00   \n",
              "49       0.004922      0.000166         0.002649        0.000085       175.00   \n",
              "50       0.006534      0.002838         0.003050        0.000563       200.00   \n",
              "\n",
              "             params  split0_test_score  split1_test_score  split2_test_score  \\\n",
              "0   {'alpha': 0.01}           0.609882           0.604033           0.594795   \n",
              "1   {'alpha': 0.05}           0.609882           0.604032           0.594800   \n",
              "2    {'alpha': 0.1}           0.609881           0.604032           0.594806   \n",
              "3    {'alpha': 0.5}           0.609877           0.604028           0.594856   \n",
              "4    {'alpha': 0.6}           0.609876           0.604027           0.594869   \n",
              "5    {'alpha': 0.7}           0.609875           0.604026           0.594881   \n",
              "6    {'alpha': 0.8}           0.609874           0.604025           0.594894   \n",
              "7    {'alpha': 0.9}           0.609873           0.604024           0.594906   \n",
              "8      {'alpha': 1}           0.609872           0.604023           0.594919   \n",
              "9    {'alpha': 1.5}           0.609867           0.604019           0.594981   \n",
              "10     {'alpha': 5}           0.609832           0.603984           0.595405   \n",
              "11     {'alpha': 6}           0.609821           0.603974           0.595524   \n",
              "12     {'alpha': 7}           0.609811           0.603964           0.595641   \n",
              "13     {'alpha': 8}           0.609801           0.603954           0.595756   \n",
              "14     {'alpha': 9}           0.609790           0.603944           0.595870   \n",
              "15    {'alpha': 10}           0.609779           0.603934           0.595983   \n",
              "16    {'alpha': 11}           0.609769           0.603924           0.596095   \n",
              "17    {'alpha': 12}           0.609758           0.603914           0.596206   \n",
              "18    {'alpha': 13}           0.609747           0.603904           0.596315   \n",
              "19    {'alpha': 14}           0.609737           0.603893           0.596423   \n",
              "20    {'alpha': 15}           0.609726           0.603883           0.596530   \n",
              "21    {'alpha': 16}           0.609715           0.603872           0.596635   \n",
              "22    {'alpha': 17}           0.609704           0.603862           0.596740   \n",
              "23    {'alpha': 18}           0.609693           0.603851           0.596843   \n",
              "24    {'alpha': 19}           0.609682           0.603841           0.596945   \n",
              "25    {'alpha': 20}           0.609671           0.603830           0.597046   \n",
              "26   {'alpha': 325}           0.605819           0.600046           0.606278   \n",
              "27    {'alpha': 30}           0.609557           0.603721           0.597998   \n",
              "28    {'alpha': 35}           0.609499           0.603665           0.598436   \n",
              "29    {'alpha': 40}           0.609440           0.603609           0.598852   \n",
              "30    {'alpha': 45}           0.609380           0.603551           0.599246   \n",
              "31    {'alpha': 50}           0.609319           0.603493           0.599619   \n",
              "32    {'alpha': 60}           0.609196           0.603374           0.600310   \n",
              "33    {'alpha': 70}           0.609071           0.603253           0.600933   \n",
              "34    {'alpha': 80}           0.608944           0.603131           0.601495   \n",
              "35    {'alpha': 90}           0.608816           0.603007           0.602004   \n",
              "36    {'alpha': 91}           0.608803           0.602994           0.602052   \n",
              "37    {'alpha': 92}           0.608790           0.602982           0.602099   \n",
              "38    {'alpha': 93}           0.608777           0.602969           0.602146   \n",
              "39    {'alpha': 94}           0.608764           0.602957           0.602193   \n",
              "40    {'alpha': 95}           0.608751           0.602944           0.602239   \n",
              "41    {'alpha': 96}           0.608738           0.602932           0.602285   \n",
              "42    {'alpha': 97}           0.608725           0.602919           0.602330   \n",
              "43    {'alpha': 98}           0.608712           0.602907           0.602375   \n",
              "44    {'alpha': 99}           0.608699           0.602894           0.602419   \n",
              "45   {'alpha': 100}           0.608686           0.602881           0.602463   \n",
              "46   {'alpha': 110}           0.608556           0.602755           0.602879   \n",
              "47   {'alpha': 125}           0.608361           0.602565           0.603431   \n",
              "48   {'alpha': 150}           0.608034           0.602246           0.604187   \n",
              "49   {'alpha': 175}           0.607708           0.601926           0.604777   \n",
              "50   {'alpha': 200}           0.607384           0.601607           0.605236   \n",
              "\n",
              "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
              "0            0.589386           0.622811         0.604181        0.011718   \n",
              "1            0.589386           0.622811         0.604182        0.011717   \n",
              "2            0.589386           0.622810         0.604183        0.011716   \n",
              "3            0.589389           0.622803         0.604191        0.011705   \n",
              "4            0.589390           0.622801         0.604193        0.011702   \n",
              "5            0.589391           0.622799         0.604195        0.011699   \n",
              "6            0.589391           0.622798         0.604197        0.011696   \n",
              "7            0.589392           0.622796         0.604198        0.011693   \n",
              "8            0.589393           0.622794         0.604200        0.011691   \n",
              "9            0.589396           0.622786         0.604210        0.011677   \n",
              "10           0.589419           0.622725         0.604273        0.011582   \n",
              "11           0.589425           0.622708         0.604291        0.011556   \n",
              "12           0.589431           0.622691         0.604308        0.011531   \n",
              "13           0.589437           0.622673         0.604324        0.011505   \n",
              "14           0.589442           0.622656         0.604341        0.011480   \n",
              "15           0.589448           0.622639         0.604357        0.011456   \n",
              "16           0.589453           0.622621         0.604372        0.011432   \n",
              "17           0.589458           0.622604         0.604388        0.011408   \n",
              "18           0.589462           0.622586         0.604403        0.011385   \n",
              "19           0.589467           0.622569         0.604418        0.011362   \n",
              "20           0.589471           0.622551         0.604432        0.011339   \n",
              "21           0.589475           0.622534         0.604446        0.011317   \n",
              "22           0.589479           0.622516         0.604460        0.011295   \n",
              "23           0.589483           0.622499         0.604474        0.011274   \n",
              "24           0.589487           0.622481         0.604487        0.011252   \n",
              "25           0.589490           0.622463         0.604500        0.011231   \n",
              "26           0.587481           0.617460         0.603417        0.009762   \n",
              "27           0.589516           0.622287         0.604616        0.011039   \n",
              "28           0.589523           0.622198         0.604664        0.010953   \n",
              "29           0.589527           0.622109         0.604707        0.010874   \n",
              "30           0.589527           0.622020         0.604745        0.010800   \n",
              "31           0.589524           0.621931         0.604777        0.010731   \n",
              "32           0.589511           0.621753         0.604829        0.010608   \n",
              "33           0.589487           0.621575         0.604864        0.010502   \n",
              "34           0.589455           0.621398         0.604884        0.010410   \n",
              "35           0.589414           0.621222         0.604892        0.010330   \n",
              "36           0.589410           0.621204         0.604892        0.010322   \n",
              "37           0.589405           0.621186         0.604892        0.010315   \n",
              "38           0.589401           0.621169         0.604892        0.010308   \n",
              "39           0.589396           0.621151         0.604892        0.010301   \n",
              "40           0.589391           0.621134         0.604892        0.010294   \n",
              "41           0.589387           0.621116         0.604891        0.010287   \n",
              "42           0.589382           0.621099         0.604891        0.010280   \n",
              "43           0.589377           0.621081         0.604890        0.010273   \n",
              "44           0.589372           0.621063         0.604890        0.010266   \n",
              "45           0.589367           0.621046         0.604889        0.010260   \n",
              "46           0.589313           0.620872         0.604875        0.010199   \n",
              "47           0.589223           0.620612         0.604838        0.010121   \n",
              "48           0.589051           0.620187         0.604741        0.010021   \n",
              "49           0.588858           0.619770         0.604608        0.009948   \n",
              "50           0.588650           0.619363         0.604448        0.009893   \n",
              "\n",
              "    rank_test_score  \n",
              "0                50  \n",
              "1                49  \n",
              "2                48  \n",
              "3                47  \n",
              "4                46  \n",
              "5                45  \n",
              "6                44  \n",
              "7                43  \n",
              "8                42  \n",
              "9                41  \n",
              "10               40  \n",
              "11               39  \n",
              "12               38  \n",
              "13               37  \n",
              "14               36  \n",
              "15               35  \n",
              "16               34  \n",
              "17               33  \n",
              "18               32  \n",
              "19               31  \n",
              "20               30  \n",
              "21               29  \n",
              "22               27  \n",
              "23               26  \n",
              "24               25  \n",
              "25               24  \n",
              "26               51  \n",
              "27               22  \n",
              "28               21  \n",
              "29               20  \n",
              "30               18  \n",
              "31               17  \n",
              "32               16  \n",
              "33               14  \n",
              "34               12  \n",
              "35                4  \n",
              "36                2  \n",
              "37                1  \n",
              "38                3  \n",
              "39                5  \n",
              "40                6  \n",
              "41                7  \n",
              "42                8  \n",
              "43                9  \n",
              "44               10  \n",
              "45               11  \n",
              "46               13  \n",
              "47               15  \n",
              "48               19  \n",
              "49               23  \n",
              "50               28  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb59b52c-92d7-467b-860e-2ee47593f201\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_alpha</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split3_test_score</th>\n",
              "      <th>split4_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.005677</td>\n",
              "      <td>0.001539</td>\n",
              "      <td>0.002475</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.01</td>\n",
              "      <td>{'alpha': 0.01}</td>\n",
              "      <td>0.609882</td>\n",
              "      <td>0.604033</td>\n",
              "      <td>0.594795</td>\n",
              "      <td>0.589386</td>\n",
              "      <td>0.622811</td>\n",
              "      <td>0.604181</td>\n",
              "      <td>0.011718</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.004404</td>\n",
              "      <td>0.000071</td>\n",
              "      <td>0.002369</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.05</td>\n",
              "      <td>{'alpha': 0.05}</td>\n",
              "      <td>0.609882</td>\n",
              "      <td>0.604032</td>\n",
              "      <td>0.594800</td>\n",
              "      <td>0.589386</td>\n",
              "      <td>0.622811</td>\n",
              "      <td>0.604182</td>\n",
              "      <td>0.011717</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.005297</td>\n",
              "      <td>0.001720</td>\n",
              "      <td>0.002424</td>\n",
              "      <td>0.000128</td>\n",
              "      <td>0.10</td>\n",
              "      <td>{'alpha': 0.1}</td>\n",
              "      <td>0.609881</td>\n",
              "      <td>0.604032</td>\n",
              "      <td>0.594806</td>\n",
              "      <td>0.589386</td>\n",
              "      <td>0.622810</td>\n",
              "      <td>0.604183</td>\n",
              "      <td>0.011716</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.004544</td>\n",
              "      <td>0.000110</td>\n",
              "      <td>0.002454</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.50</td>\n",
              "      <td>{'alpha': 0.5}</td>\n",
              "      <td>0.609877</td>\n",
              "      <td>0.604028</td>\n",
              "      <td>0.594856</td>\n",
              "      <td>0.589389</td>\n",
              "      <td>0.622803</td>\n",
              "      <td>0.604191</td>\n",
              "      <td>0.011705</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.004199</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.002292</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.60</td>\n",
              "      <td>{'alpha': 0.6}</td>\n",
              "      <td>0.609876</td>\n",
              "      <td>0.604027</td>\n",
              "      <td>0.594869</td>\n",
              "      <td>0.589390</td>\n",
              "      <td>0.622801</td>\n",
              "      <td>0.604193</td>\n",
              "      <td>0.011702</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.004152</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.002246</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.70</td>\n",
              "      <td>{'alpha': 0.7}</td>\n",
              "      <td>0.609875</td>\n",
              "      <td>0.604026</td>\n",
              "      <td>0.594881</td>\n",
              "      <td>0.589391</td>\n",
              "      <td>0.622799</td>\n",
              "      <td>0.604195</td>\n",
              "      <td>0.011699</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.004183</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.002318</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.80</td>\n",
              "      <td>{'alpha': 0.8}</td>\n",
              "      <td>0.609874</td>\n",
              "      <td>0.604025</td>\n",
              "      <td>0.594894</td>\n",
              "      <td>0.589391</td>\n",
              "      <td>0.622798</td>\n",
              "      <td>0.604197</td>\n",
              "      <td>0.011696</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.004026</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.002168</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.90</td>\n",
              "      <td>{'alpha': 0.9}</td>\n",
              "      <td>0.609873</td>\n",
              "      <td>0.604024</td>\n",
              "      <td>0.594906</td>\n",
              "      <td>0.589392</td>\n",
              "      <td>0.622796</td>\n",
              "      <td>0.604198</td>\n",
              "      <td>0.011693</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.003961</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>0.002166</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>1.00</td>\n",
              "      <td>{'alpha': 1}</td>\n",
              "      <td>0.609872</td>\n",
              "      <td>0.604023</td>\n",
              "      <td>0.594919</td>\n",
              "      <td>0.589393</td>\n",
              "      <td>0.622794</td>\n",
              "      <td>0.604200</td>\n",
              "      <td>0.011691</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.004106</td>\n",
              "      <td>0.000103</td>\n",
              "      <td>0.002160</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>1.50</td>\n",
              "      <td>{'alpha': 1.5}</td>\n",
              "      <td>0.609867</td>\n",
              "      <td>0.604019</td>\n",
              "      <td>0.594981</td>\n",
              "      <td>0.589396</td>\n",
              "      <td>0.622786</td>\n",
              "      <td>0.604210</td>\n",
              "      <td>0.011677</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.003929</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.002099</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>5.00</td>\n",
              "      <td>{'alpha': 5}</td>\n",
              "      <td>0.609832</td>\n",
              "      <td>0.603984</td>\n",
              "      <td>0.595405</td>\n",
              "      <td>0.589419</td>\n",
              "      <td>0.622725</td>\n",
              "      <td>0.604273</td>\n",
              "      <td>0.011582</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.003974</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.002122</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>6.00</td>\n",
              "      <td>{'alpha': 6}</td>\n",
              "      <td>0.609821</td>\n",
              "      <td>0.603974</td>\n",
              "      <td>0.595524</td>\n",
              "      <td>0.589425</td>\n",
              "      <td>0.622708</td>\n",
              "      <td>0.604291</td>\n",
              "      <td>0.011556</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.004137</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.002281</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>7.00</td>\n",
              "      <td>{'alpha': 7}</td>\n",
              "      <td>0.609811</td>\n",
              "      <td>0.603964</td>\n",
              "      <td>0.595641</td>\n",
              "      <td>0.589431</td>\n",
              "      <td>0.622691</td>\n",
              "      <td>0.604308</td>\n",
              "      <td>0.011531</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.004151</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>0.002185</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>8.00</td>\n",
              "      <td>{'alpha': 8}</td>\n",
              "      <td>0.609801</td>\n",
              "      <td>0.603954</td>\n",
              "      <td>0.595756</td>\n",
              "      <td>0.589437</td>\n",
              "      <td>0.622673</td>\n",
              "      <td>0.604324</td>\n",
              "      <td>0.011505</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.004324</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.002294</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>9.00</td>\n",
              "      <td>{'alpha': 9}</td>\n",
              "      <td>0.609790</td>\n",
              "      <td>0.603944</td>\n",
              "      <td>0.595870</td>\n",
              "      <td>0.589442</td>\n",
              "      <td>0.622656</td>\n",
              "      <td>0.604341</td>\n",
              "      <td>0.011480</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.004521</td>\n",
              "      <td>0.000681</td>\n",
              "      <td>0.002453</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>10.00</td>\n",
              "      <td>{'alpha': 10}</td>\n",
              "      <td>0.609779</td>\n",
              "      <td>0.603934</td>\n",
              "      <td>0.595983</td>\n",
              "      <td>0.589448</td>\n",
              "      <td>0.622639</td>\n",
              "      <td>0.604357</td>\n",
              "      <td>0.011456</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.004187</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.002239</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>11.00</td>\n",
              "      <td>{'alpha': 11}</td>\n",
              "      <td>0.609769</td>\n",
              "      <td>0.603924</td>\n",
              "      <td>0.596095</td>\n",
              "      <td>0.589453</td>\n",
              "      <td>0.622621</td>\n",
              "      <td>0.604372</td>\n",
              "      <td>0.011432</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.004191</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.002197</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>12.00</td>\n",
              "      <td>{'alpha': 12}</td>\n",
              "      <td>0.609758</td>\n",
              "      <td>0.603914</td>\n",
              "      <td>0.596206</td>\n",
              "      <td>0.589458</td>\n",
              "      <td>0.622604</td>\n",
              "      <td>0.604388</td>\n",
              "      <td>0.011408</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.004125</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.002163</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>13.00</td>\n",
              "      <td>{'alpha': 13}</td>\n",
              "      <td>0.609747</td>\n",
              "      <td>0.603904</td>\n",
              "      <td>0.596315</td>\n",
              "      <td>0.589462</td>\n",
              "      <td>0.622586</td>\n",
              "      <td>0.604403</td>\n",
              "      <td>0.011385</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.004158</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.002293</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>14.00</td>\n",
              "      <td>{'alpha': 14}</td>\n",
              "      <td>0.609737</td>\n",
              "      <td>0.603893</td>\n",
              "      <td>0.596423</td>\n",
              "      <td>0.589467</td>\n",
              "      <td>0.622569</td>\n",
              "      <td>0.604418</td>\n",
              "      <td>0.011362</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.004193</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>0.002313</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>15.00</td>\n",
              "      <td>{'alpha': 15}</td>\n",
              "      <td>0.609726</td>\n",
              "      <td>0.603883</td>\n",
              "      <td>0.596530</td>\n",
              "      <td>0.589471</td>\n",
              "      <td>0.622551</td>\n",
              "      <td>0.604432</td>\n",
              "      <td>0.011339</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.004875</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>0.003335</td>\n",
              "      <td>0.001316</td>\n",
              "      <td>16.00</td>\n",
              "      <td>{'alpha': 16}</td>\n",
              "      <td>0.609715</td>\n",
              "      <td>0.603872</td>\n",
              "      <td>0.596635</td>\n",
              "      <td>0.589475</td>\n",
              "      <td>0.622534</td>\n",
              "      <td>0.604446</td>\n",
              "      <td>0.011317</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.005811</td>\n",
              "      <td>0.001476</td>\n",
              "      <td>0.002596</td>\n",
              "      <td>0.000576</td>\n",
              "      <td>17.00</td>\n",
              "      <td>{'alpha': 17}</td>\n",
              "      <td>0.609704</td>\n",
              "      <td>0.603862</td>\n",
              "      <td>0.596740</td>\n",
              "      <td>0.589479</td>\n",
              "      <td>0.622516</td>\n",
              "      <td>0.604460</td>\n",
              "      <td>0.011295</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.005102</td>\n",
              "      <td>0.000961</td>\n",
              "      <td>0.003399</td>\n",
              "      <td>0.000871</td>\n",
              "      <td>18.00</td>\n",
              "      <td>{'alpha': 18}</td>\n",
              "      <td>0.609693</td>\n",
              "      <td>0.603851</td>\n",
              "      <td>0.596843</td>\n",
              "      <td>0.589483</td>\n",
              "      <td>0.622499</td>\n",
              "      <td>0.604474</td>\n",
              "      <td>0.011274</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.004732</td>\n",
              "      <td>0.000904</td>\n",
              "      <td>0.002269</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>19.00</td>\n",
              "      <td>{'alpha': 19}</td>\n",
              "      <td>0.609682</td>\n",
              "      <td>0.603841</td>\n",
              "      <td>0.596945</td>\n",
              "      <td>0.589487</td>\n",
              "      <td>0.622481</td>\n",
              "      <td>0.604487</td>\n",
              "      <td>0.011252</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.004635</td>\n",
              "      <td>0.000839</td>\n",
              "      <td>0.002275</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>20.00</td>\n",
              "      <td>{'alpha': 20}</td>\n",
              "      <td>0.609671</td>\n",
              "      <td>0.603830</td>\n",
              "      <td>0.597046</td>\n",
              "      <td>0.589490</td>\n",
              "      <td>0.622463</td>\n",
              "      <td>0.604500</td>\n",
              "      <td>0.011231</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.004658</td>\n",
              "      <td>0.000660</td>\n",
              "      <td>0.002236</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>325.00</td>\n",
              "      <td>{'alpha': 325}</td>\n",
              "      <td>0.605819</td>\n",
              "      <td>0.600046</td>\n",
              "      <td>0.606278</td>\n",
              "      <td>0.587481</td>\n",
              "      <td>0.617460</td>\n",
              "      <td>0.603417</td>\n",
              "      <td>0.009762</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.004244</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.002247</td>\n",
              "      <td>0.000059</td>\n",
              "      <td>30.00</td>\n",
              "      <td>{'alpha': 30}</td>\n",
              "      <td>0.609557</td>\n",
              "      <td>0.603721</td>\n",
              "      <td>0.597998</td>\n",
              "      <td>0.589516</td>\n",
              "      <td>0.622287</td>\n",
              "      <td>0.604616</td>\n",
              "      <td>0.011039</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.004172</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>0.002168</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>35.00</td>\n",
              "      <td>{'alpha': 35}</td>\n",
              "      <td>0.609499</td>\n",
              "      <td>0.603665</td>\n",
              "      <td>0.598436</td>\n",
              "      <td>0.589523</td>\n",
              "      <td>0.622198</td>\n",
              "      <td>0.604664</td>\n",
              "      <td>0.010953</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.004108</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>0.002215</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>40.00</td>\n",
              "      <td>{'alpha': 40}</td>\n",
              "      <td>0.609440</td>\n",
              "      <td>0.603609</td>\n",
              "      <td>0.598852</td>\n",
              "      <td>0.589527</td>\n",
              "      <td>0.622109</td>\n",
              "      <td>0.604707</td>\n",
              "      <td>0.010874</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.004367</td>\n",
              "      <td>0.000247</td>\n",
              "      <td>0.002205</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>45.00</td>\n",
              "      <td>{'alpha': 45}</td>\n",
              "      <td>0.609380</td>\n",
              "      <td>0.603551</td>\n",
              "      <td>0.599246</td>\n",
              "      <td>0.589527</td>\n",
              "      <td>0.622020</td>\n",
              "      <td>0.604745</td>\n",
              "      <td>0.010800</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.004214</td>\n",
              "      <td>0.000117</td>\n",
              "      <td>0.002256</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>50.00</td>\n",
              "      <td>{'alpha': 50}</td>\n",
              "      <td>0.609319</td>\n",
              "      <td>0.603493</td>\n",
              "      <td>0.599619</td>\n",
              "      <td>0.589524</td>\n",
              "      <td>0.621931</td>\n",
              "      <td>0.604777</td>\n",
              "      <td>0.010731</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.004452</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.002380</td>\n",
              "      <td>0.000059</td>\n",
              "      <td>60.00</td>\n",
              "      <td>{'alpha': 60}</td>\n",
              "      <td>0.609196</td>\n",
              "      <td>0.603374</td>\n",
              "      <td>0.600310</td>\n",
              "      <td>0.589511</td>\n",
              "      <td>0.621753</td>\n",
              "      <td>0.604829</td>\n",
              "      <td>0.010608</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.004497</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>0.002884</td>\n",
              "      <td>0.000637</td>\n",
              "      <td>70.00</td>\n",
              "      <td>{'alpha': 70}</td>\n",
              "      <td>0.609071</td>\n",
              "      <td>0.603253</td>\n",
              "      <td>0.600933</td>\n",
              "      <td>0.589487</td>\n",
              "      <td>0.621575</td>\n",
              "      <td>0.604864</td>\n",
              "      <td>0.010502</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.005124</td>\n",
              "      <td>0.000537</td>\n",
              "      <td>0.002636</td>\n",
              "      <td>0.000090</td>\n",
              "      <td>80.00</td>\n",
              "      <td>{'alpha': 80}</td>\n",
              "      <td>0.608944</td>\n",
              "      <td>0.603131</td>\n",
              "      <td>0.601495</td>\n",
              "      <td>0.589455</td>\n",
              "      <td>0.621398</td>\n",
              "      <td>0.604884</td>\n",
              "      <td>0.010410</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.004754</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.003021</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>90.00</td>\n",
              "      <td>{'alpha': 90}</td>\n",
              "      <td>0.608816</td>\n",
              "      <td>0.603007</td>\n",
              "      <td>0.602004</td>\n",
              "      <td>0.589414</td>\n",
              "      <td>0.621222</td>\n",
              "      <td>0.604892</td>\n",
              "      <td>0.010330</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.004565</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.002439</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>91.00</td>\n",
              "      <td>{'alpha': 91}</td>\n",
              "      <td>0.608803</td>\n",
              "      <td>0.602994</td>\n",
              "      <td>0.602052</td>\n",
              "      <td>0.589410</td>\n",
              "      <td>0.621204</td>\n",
              "      <td>0.604892</td>\n",
              "      <td>0.010322</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.004197</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>0.002211</td>\n",
              "      <td>0.000059</td>\n",
              "      <td>92.00</td>\n",
              "      <td>{'alpha': 92}</td>\n",
              "      <td>0.608790</td>\n",
              "      <td>0.602982</td>\n",
              "      <td>0.602099</td>\n",
              "      <td>0.589405</td>\n",
              "      <td>0.621186</td>\n",
              "      <td>0.604892</td>\n",
              "      <td>0.010315</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.004389</td>\n",
              "      <td>0.000384</td>\n",
              "      <td>0.002363</td>\n",
              "      <td>0.000179</td>\n",
              "      <td>93.00</td>\n",
              "      <td>{'alpha': 93}</td>\n",
              "      <td>0.608777</td>\n",
              "      <td>0.602969</td>\n",
              "      <td>0.602146</td>\n",
              "      <td>0.589401</td>\n",
              "      <td>0.621169</td>\n",
              "      <td>0.604892</td>\n",
              "      <td>0.010308</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.004463</td>\n",
              "      <td>0.000333</td>\n",
              "      <td>0.002331</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>94.00</td>\n",
              "      <td>{'alpha': 94}</td>\n",
              "      <td>0.608764</td>\n",
              "      <td>0.602957</td>\n",
              "      <td>0.602193</td>\n",
              "      <td>0.589396</td>\n",
              "      <td>0.621151</td>\n",
              "      <td>0.604892</td>\n",
              "      <td>0.010301</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.004116</td>\n",
              "      <td>0.000123</td>\n",
              "      <td>0.002205</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>95.00</td>\n",
              "      <td>{'alpha': 95}</td>\n",
              "      <td>0.608751</td>\n",
              "      <td>0.602944</td>\n",
              "      <td>0.602239</td>\n",
              "      <td>0.589391</td>\n",
              "      <td>0.621134</td>\n",
              "      <td>0.604892</td>\n",
              "      <td>0.010294</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.004552</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>0.002519</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>96.00</td>\n",
              "      <td>{'alpha': 96}</td>\n",
              "      <td>0.608738</td>\n",
              "      <td>0.602932</td>\n",
              "      <td>0.602285</td>\n",
              "      <td>0.589387</td>\n",
              "      <td>0.621116</td>\n",
              "      <td>0.604891</td>\n",
              "      <td>0.010287</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.004664</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.002740</td>\n",
              "      <td>0.000461</td>\n",
              "      <td>97.00</td>\n",
              "      <td>{'alpha': 97}</td>\n",
              "      <td>0.608725</td>\n",
              "      <td>0.602919</td>\n",
              "      <td>0.602330</td>\n",
              "      <td>0.589382</td>\n",
              "      <td>0.621099</td>\n",
              "      <td>0.604891</td>\n",
              "      <td>0.010280</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.005106</td>\n",
              "      <td>0.000464</td>\n",
              "      <td>0.002634</td>\n",
              "      <td>0.000082</td>\n",
              "      <td>98.00</td>\n",
              "      <td>{'alpha': 98}</td>\n",
              "      <td>0.608712</td>\n",
              "      <td>0.602907</td>\n",
              "      <td>0.602375</td>\n",
              "      <td>0.589377</td>\n",
              "      <td>0.621081</td>\n",
              "      <td>0.604890</td>\n",
              "      <td>0.010273</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.004784</td>\n",
              "      <td>0.000293</td>\n",
              "      <td>0.002513</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>99.00</td>\n",
              "      <td>{'alpha': 99}</td>\n",
              "      <td>0.608699</td>\n",
              "      <td>0.602894</td>\n",
              "      <td>0.602419</td>\n",
              "      <td>0.589372</td>\n",
              "      <td>0.621063</td>\n",
              "      <td>0.604890</td>\n",
              "      <td>0.010266</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.004747</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.002738</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>100.00</td>\n",
              "      <td>{'alpha': 100}</td>\n",
              "      <td>0.608686</td>\n",
              "      <td>0.602881</td>\n",
              "      <td>0.602463</td>\n",
              "      <td>0.589367</td>\n",
              "      <td>0.621046</td>\n",
              "      <td>0.604889</td>\n",
              "      <td>0.010260</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.004535</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.002527</td>\n",
              "      <td>0.000179</td>\n",
              "      <td>110.00</td>\n",
              "      <td>{'alpha': 110}</td>\n",
              "      <td>0.608556</td>\n",
              "      <td>0.602755</td>\n",
              "      <td>0.602879</td>\n",
              "      <td>0.589313</td>\n",
              "      <td>0.620872</td>\n",
              "      <td>0.604875</td>\n",
              "      <td>0.010199</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.005942</td>\n",
              "      <td>0.001890</td>\n",
              "      <td>0.002733</td>\n",
              "      <td>0.000299</td>\n",
              "      <td>125.00</td>\n",
              "      <td>{'alpha': 125}</td>\n",
              "      <td>0.608361</td>\n",
              "      <td>0.602565</td>\n",
              "      <td>0.603431</td>\n",
              "      <td>0.589223</td>\n",
              "      <td>0.620612</td>\n",
              "      <td>0.604838</td>\n",
              "      <td>0.010121</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.004887</td>\n",
              "      <td>0.000212</td>\n",
              "      <td>0.002578</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>150.00</td>\n",
              "      <td>{'alpha': 150}</td>\n",
              "      <td>0.608034</td>\n",
              "      <td>0.602246</td>\n",
              "      <td>0.604187</td>\n",
              "      <td>0.589051</td>\n",
              "      <td>0.620187</td>\n",
              "      <td>0.604741</td>\n",
              "      <td>0.010021</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.004922</td>\n",
              "      <td>0.000166</td>\n",
              "      <td>0.002649</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>175.00</td>\n",
              "      <td>{'alpha': 175}</td>\n",
              "      <td>0.607708</td>\n",
              "      <td>0.601926</td>\n",
              "      <td>0.604777</td>\n",
              "      <td>0.588858</td>\n",
              "      <td>0.619770</td>\n",
              "      <td>0.604608</td>\n",
              "      <td>0.009948</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0.006534</td>\n",
              "      <td>0.002838</td>\n",
              "      <td>0.003050</td>\n",
              "      <td>0.000563</td>\n",
              "      <td>200.00</td>\n",
              "      <td>{'alpha': 200}</td>\n",
              "      <td>0.607384</td>\n",
              "      <td>0.601607</td>\n",
              "      <td>0.605236</td>\n",
              "      <td>0.588650</td>\n",
              "      <td>0.619363</td>\n",
              "      <td>0.604448</td>\n",
              "      <td>0.009893</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb59b52c-92d7-467b-860e-2ee47593f201')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bb59b52c-92d7-467b-860e-2ee47593f201 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bb59b52c-92d7-467b-860e-2ee47593f201');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-10e5551c-baf1-4aa8-ba4e-ba82009ada8e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-10e5551c-baf1-4aa8-ba4e-ba82009ada8e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-10e5551c-baf1-4aa8-ba4e-ba82009ada8e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 51,\n  \"fields\": [\n    {\n      \"column\": \"mean_fit_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0005440801130222722,\n        \"min\": 0.00392913818359375,\n        \"max\": 0.006534099578857422,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          0.005106115341186523,\n          0.004115915298461914,\n          0.004534578323364258\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std_fit_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0005718707922224444,\n        \"min\": 3.086378003943138e-05,\n        \"max\": 0.002838040622372634,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          0.0004643948786277052,\n          0.00012330778419246318,\n          0.0003121221604389051\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_score_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0002967917294316216,\n        \"min\": 0.0020985126495361326,\n        \"max\": 0.0033994674682617187,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          0.0026337623596191405,\n          0.002204561233520508,\n          0.002527332305908203\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std_score_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0002626870531285561,\n        \"min\": 4.0556394910745465e-06,\n        \"max\": 0.0013155354832241175,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          8.156026042799126e-05,\n          5.31923765427769e-05,\n          0.0001788211566738051\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"param_alpha\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 63.87258056351832,\n        \"min\": 0.01,\n        \"max\": 325.0,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          98.0,\n          95.0,\n          110.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"params\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split0_test_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0007934394985268721,\n        \"min\": 0.6058188896211786,\n        \"max\": 0.6098821484196719,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          0.6087121752830105,\n          0.6087509918865722,\n          0.6085563105922567\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split1_test_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0007722364083295074,\n        \"min\": 0.6000459458490226,\n        \"max\": 0.6040328298383684,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          0.6029065815153486,\n          0.6029441910745392,\n          0.6027553501190326\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split2_test_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0034893374774111557,\n        \"min\": 0.5947947847179897,\n        \"max\": 0.606278497042152,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          0.6023749512118599,\n          0.6022391415526062,\n          0.6028793892915851\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split3_test_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00030873362616844354,\n        \"min\": 0.5874813243032058,\n        \"max\": 0.589527037867315,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          0.5893767944273309,\n          0.5893912840943429,\n          0.5893134013262644\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split4_test_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0010900853790821734,\n        \"min\": 0.6174600392406004,\n        \"max\": 0.622811338678332,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          0.6210810021912061,\n          0.6211336154972464,\n          0.6208715471683021\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_test_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0003148282882606678,\n        \"min\": 0.6034169392112319,\n        \"max\": 0.60489243063517,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          0.6048903009257512,\n          0.6048918448210614,\n          0.6048751996994881\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std_test_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0006349696429849911,\n        \"min\": 0.009761905102707748,\n        \"max\": 0.011718475823844762,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          0.010273037932654468,\n          0.010293548497583589,\n          0.01019882640096211\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rank_test_score\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 51,\n        \"samples\": [\n          9,\n          6,\n          13\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's finally use the testing set! Use the output of `GridSearchCV` to find the score on the test set. Does your model generalize well? *(3 points)*"
      ],
      "metadata": {
        "id": "7H-OpKcHj_Ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid_searched_test=GridSearchCV(Ridge(),params2,cv=5)\n",
        "grid_searched_test.fit(X_trainval,Y_trainval)\n",
        "print(grid_searched_test.score(X_test, Y_test))\n",
        "print(grid_searched_test.best_score_)\n",
        "print(grid_searched_test.best_params_)\n"
      ],
      "metadata": {
        "id": "r8LMBJWnU7Aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0fb85d-f168-4247-e35d-f85cbc2cc07b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.595626306755606\n",
            "0.60489243063517\n",
            "{'alpha': 92}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It does not generalize because it has a very low accuracy throught the test and training sets."
      ],
      "metadata": {
        "id": "NTdDNXTfwfp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3: Group K-Fold *(17 points)*"
      ],
      "metadata": {
        "id": "2UsfCauqkQzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many data sets are often stored already in a training and test set. Data was originally collected (training) and then another collection occured (testing) that could then be used to evaluate a model. In the UCI HAR Dataset, we need to download the inputs and outputs of the training set.\n",
        "\n",
        "Both are stored without column titles, so we need to include `header=None` to let Python know there are no column titles.\n",
        "\n",
        "Similarly, because the data is stored as a .txt file, all input values are separated by a whitespace, so we need to tell Python how to separte column values. The `ravel` function used on the output values makes the many univariate observations now stored as an array."
      ],
      "metadata": {
        "id": "RmyplDIjlnxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You will need to replace \"YOUR PATH\" with the path to your txt files relative to where you ipynb file is saved\n",
        "import pandas as pd\n",
        "X_tr = pd.read_csv('drive/MyDrive/UCI HAR Dataset/UCI HAR Dataset/train/X_train.txt',header=None,delim_whitespace=True)\n",
        "Y_tr = pd.read_csv('drive/MyDrive/UCI HAR Dataset/UCI HAR Dataset/train/y_train.txt',header=None).values.ravel()\n"
      ],
      "metadata": {
        "id": "_vJMynK6Y_Ut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e8b0ef-89f8-42a1-86e2-a17d55bfab46"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-c0cb46cc9e47>:3: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  X_tr = pd.read_csv('drive/MyDrive/UCI HAR Dataset/UCI HAR Dataset/train/X_train.txt',header=None,delim_whitespace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that if we are classifying data points that are similar to one another, it would be better if we ensure that every \"similar\" point is put together (either in the training set or the test set).\n",
        "\n",
        "The UCI HAR Dataset contains recordings of 30 individuals performing activities of daily living while carrying a waist-mounted smartphone with embedded inertial sensors. The classifier is tasked with classifying the human activity that was performed.\n",
        "\n",
        "The following code provides the grouping information for each of the individuals."
      ],
      "metadata": {
        "id": "HdKYfjx4mZlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You will need to replace \"YOUR PATH\" with the path to your txt files relative to where you ipynb file is saved\n",
        "\n",
        "group = pd.read_csv('drive/MyDrive/UCI HAR Dataset/UCI HAR Dataset/train/subject_train.txt',header=None).values.ravel()"
      ],
      "metadata": {
        "id": "qs0XaZRdnLtR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a 10-Fold stratified cross-validation for a Random Forest model with the inputs `n_jobs=-1` and `n_estimators=20`. Return the average accuracy. *(2 points)*"
      ],
      "metadata": {
        "id": "qiXCOk5-nNzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "forest_txt = RandomForestClassifier(n_jobs=1,n_estimators=20)\n"
      ],
      "metadata": {
        "id": "DIUJAvX-bdWe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "scored=cross_val_score(forest_txt,X_tr, Y_tr, cv=10)\n",
        "txt_mean=np.mean(scored)\n",
        "print(txt_mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dReLyczK1LWN",
        "outputId": "7c13570d-67ee-415d-b2d4-1a582fdf9840"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9227465986394557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, use a Group 10-Fold stratified cross-validation using a Random Forest model with the same inputs as before. Reutn the average accuracy. *(6 points)*"
      ],
      "metadata": {
        "id": "5PT1GKn8nwmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GroupKFold\n",
        "gf = GroupKFold(n_splits=10)\n",
        "group_score=cross_val_score(forest_txt,X_tr,Y_tr,groups=group,cv=gf)\n",
        "txt_mean_group=np.mean(group_score)\n",
        "print(txt_mean_group)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXU-pOWD4arE",
        "outputId": "b80f63d9-3836-41a6-8014-af0b89e36888"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.900890068176426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may notice that the ungrouped gave you higher accuracy than the grouped version. This is because we have overfit our data. Give an explanation of what is happening and why the first accuracy is too optimistic of an accuracy. *(2 points)*"
      ],
      "metadata": {
        "id": "vErOtZwTn43o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is overfit in the ungrouped cross validation because it uses the entire data set on specific individuals so that data is largely following patterns within those individual's occurances without considering potential variances in the patterns across people with similar situations, making it ineffective at making predictions for new data. Thereby making it too optimistic for predicting the output for newly collect inputs."
      ],
      "metadata": {
        "id": "AIV3hZfXoX_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, load in the test sets from the UCI HAR Dataset. Build a new Random Forest model training on ALL of the training set using the same inputs and then calculate the score on the test set. For this final accuracy, explain why it is closer to the group-folded average score. *(7 points)*"
      ],
      "metadata": {
        "id": "d-L_X74qoYTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_te = pd.read_csv('drive/MyDrive/UCI HAR Dataset/UCI HAR Dataset/test/X_test.txt',header=None,delim_whitespace=True)\n",
        "Y_te = pd.read_csv('drive/MyDrive/UCI HAR Dataset/UCI HAR Dataset/test/y_test.txt',header=None).values.ravel()\n"
      ],
      "metadata": {
        "id": "SB_hxVSDiMti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79d0fe1a-74bc-41e1-b63a-0e94747771bc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-c034f5241b08>:1: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  X_te = pd.read_csv('drive/MyDrive/UCI HAR Dataset/UCI HAR Dataset/test/X_test.txt',header=None,delim_whitespace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "forest_te = RandomForestClassifier(n_jobs=1,n_estimators=20)"
      ],
      "metadata": {
        "id": "xcoaRkMKjPhN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_te=forest_te.fit(X_tr,Y_tr)"
      ],
      "metadata": {
        "id": "IS_41ggakjGu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_te.score(X_te,Y_te)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akV9d5UGsHFv",
        "outputId": "9688bac4-cedf-492b-f0ae-b4fcf2f63d96"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9189005768578216"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This accuracy is closer to the group folded because it's finding similarities between data points and characterizing patterns based on those similarities, making it similar to grouping and improving its accuracy."
      ],
      "metadata": {
        "id": "2QgDRRd7sYiK"
      }
    }
  ]
}